# Master Design Document: Market Surveillance System

## Goal
- Collect order book snapshots across many markets/tickers (rotating subscriptions), store them to Parquet on disk (Hive-style partitions), and produce basic "interestingness" metrics to help choose markets and build canonical mappings later.
- Defer any real-time dashboard/UI. "Just the data" plus offline mining.

## Non-goals (for this prompt)
- Trading, order placement, execution
- Redis/Kafka infrastructure
- Full canonical mapping automation

## Fixed decisions (DO NOT change)

### 1) Storage layout (Hive partitions, time-bucketed files, avoid small files):
```
data/orderbook_snapshots/
  venue=polymarket/date=YYYY-MM-DD/hour=HH/snapshots_YYYY-MM-DDTHH-mm.parquet
  venue=kalshi/date=YYYY-MM-DD/hour=HH/snapshots_YYYY-MM-DDTHH-mm.parquet
```
- File bucket is 5 minutes by default (HH-mm is bucket start).
- Write atomically: write temp then rename.

### 2) Order book representation (Option B):
- Store book levels as list columns in Parquet:
  `bid_px: list<f64>, bid_sz: list<f64>, ask_px: list<f64>, ask_sz: list<f64>`
- Still cap stored depth to TOP_K levels per side (configurable; default 50).

### 3) Snapshot row schema (Parquet)
**Required columns:**
- `ts_recv: i64` (epoch ms UTC)
- `venue: string`
- `market_id: string`
- `outcome_id: string`
- `seq: i64` (monotonic per market/outcome if feasible; else 0)
- `best_bid_px: f64`
- `best_bid_sz: f64`
- `best_ask_px: f64`
- `best_ask_sz: f64`
- `mid: f64`
- `spread: f64`
- `bid_px: list<f64>`
- `bid_sz: list<f64>`
- `ask_px: list<f64>`
- `ask_sz: list<f64>`

**Optional but recommended columns:**
- `status: string` ("ok"|"stale"|"partial"|"empty")
- `err: string`
- `source_ts: i64 nullable` (epoch ms), if provided by venue

### 4) Rotation strategy (implement in a simple, safe way)
- Maintain a HOT set (always subscribed) + WARM rotating set.
- Config:
  - `max_subs_per_venue` (default 200)
  - `hot_count` (default 40)
  - `rotation_period_secs` (default 180)
  - `snapshot_interval_ms_hot` (default 2000)
  - `snapshot_interval_ms_warm` (default 10000)
  - `subscription_churn_limit_per_minute` (default 20)
- Scheduler produces a target subscription set for each venue. Collector applies diffs rate-limited.

## Implementation requirements
- Use Rust stable.
- Use Polars for offline mining.
- Use Parquet writing via Arrow ecosystem (arrow2 + parquet2 is acceptable) OR a modern maintained Arrow/Parquet crate. Choose a stack that compiles cleanly on stable and is practical.
- Use tokio for async and websockets via tokio-tungstenite (or equivalent).
- Provide a single workspace with multiple bins.
- Provide clear module boundaries: scanner, scheduler, collector, storage/parquet_writer, analytics/miner, config.
- Include robust logging (tracing) and metrics counters (can be simple, e.g., exported as periodic log lines; Prometheus optional).
- Include backoff on reconnect; never reconnect storm.

## Important realism constraints
- You may not know exact Polymarket/Kalshi API shapes. That's fine.
- Implement venue adapters as traits with placeholder endpoints and message formats, and make the system compile and run in "mock mode" (simulated data) so we can validate architecture locally.
- Provide a small "mock venue" adapter that generates synthetic order books for testing end-to-end (collector → parquet → miner).
- Implement Kalshi/Polymarket adapters with stub functions and TODOs where credentials/endpoints must be inserted.

## Deliverables

### 1) A Rust workspace that builds:
- `bin: surveillance_scanner` (runs scanners)
- `bin: surveillance_collect` (runs collectors + scheduler + parquet writer)
- `bin: surveillance_miner` (runs offline mining using Polars)
- optional: `bin: surveillance_mock` (or integrate mock mode via config)

### 2) A config file:
`config/surveillance.toml`
that supports:
- data_dir
- venues enabled
- per-venue max_subs/hot_count/rotation periods
- top_k levels
- polling intervals
- API credentials placeholders
- mock mode toggles

### 3) Parquet writer:
- Batches rows in memory
- Flush rules:
  - flush when bucket changes (5-minute bucket)
  - or when row buffer exceeds N rows (config; default 50_000)
  - or after T seconds (config; default 5s)
- Atomic writes and directory creation

### 4) Scheduler:
- Input: discovered market universe (from scanner output JSONL/Parquet)
- Output: target subscription sets
- Initially implement a simple scoring:
  - if no historical stats yet: choose by recency/active status and round-robin
  - later: read a small "stats cache" file (generated by miner) to pick hot markets
- Must enforce churn limit.

### 5) Scanner:
- Produces a stable on-disk universe file per venue:
  `data/metadata/venue=.../date=YYYY-MM-DD/universe.jsonl`
- Each record includes at least:
  - market_id, outcome_ids, title, close_ts, status/tags
- In mock mode, generate a universe of configurable size.

### 6) Miner (Polars):
- Reads Parquet snapshots for a date/hour range
- Produces:
  - top markets by average depth (e.g., sum of top 3 level sizes)
  - top markets by tightest avg spread
  - most active markets by quote update count (use ts_recv deltas per market/outcome)
- Writes a compact "stats cache":
  `data/stats/venue=.../date=YYYY-MM-DD/stats.parquet`
- Print a concise summary to stdout.

## Repository layout (implement exactly)
```
services/surveillance/
  Cargo.toml (workspace member)
  src/
    lib.rs
    config.rs
    timebucket.rs
    schema.rs
    venue/
      mod.rs
      traits.rs
      mock.rs
      polymarket.rs
      kalshi.rs
    scanner/
      mod.rs
      scanner.rs
    scheduler/
      mod.rs
      scheduler.rs
      scoring.rs
    collector/
      mod.rs
      collector.rs
      book.rs
      snapshotter.rs
      subscriptions.rs
    storage/
      mod.rs
      parquet_writer.rs
      manifest.rs
    analytics/
      mod.rs
      miner.rs
bins/
  surveillance_scanner.rs
  surveillance_collect.rs
  surveillance_miner.rs
config/
  surveillance.toml
README.md
```

## Implementation detail guidance (follow these)
- Use a single canonical internal struct for a snapshot row:
  `SnapshotRow { ... fields ... , bid_px: Vec<f64>, ... }`
- Ensure bids are sorted descending price, asks ascending price.
- Compute mid/spread from best bid/ask when both present; else set mid/spread to NaN and status accordingly.
- Cap Vec lengths to TOP_K and ensure px and sz vectors match lengths.
- Parquet lists must be valid Arrow List arrays; don't store as JSON strings.
- For stability: collector should keep a per-venue in-memory map:
  `HashMap<(market_id,outcome_id), BookState>`
  `BookState` stores last update ts, current levels, seq counter.
- Snapshot frequency:
  - HOT markets: snapshot every `snapshot_interval_ms_hot`
  - WARM markets: snapshot every `snapshot_interval_ms_warm`
  Implement via a timer wheel or per-market next_due scheduling to avoid O(N) timers.
- Subscription diffs:
  - Apply subscribe first, then unsubscribe removed.
  - Rate-limit subscription messages to churn limit.
- Logging:
  - Periodically log: connected, subs count, snapshots/sec, write flush stats, queue sizes.

## Acceptance criteria
- `cargo build` succeeds.
- In mock mode:
  - scanner generates a universe
  - collector rotates subscriptions and writes Parquet snapshots to the required directory layout
  - miner reads the Parquet data and outputs ranked markets and writes stats cache parquet
- Provide clear TODOs for real API wiring, but the architecture and storage/mining pipeline must be fully implemented.
